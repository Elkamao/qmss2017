{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Processing\n",
    "## Reintroduction to Strings\n",
    "Before beginning with text processing in full, we will review and introduce several important topics in working with strings.\n",
    "### String Attributes\n",
    "Let's quickly review some useful features of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Strings, demarcated by single or double quotation markers, can be\n",
    "## joined together by the addition operator. They may also be combined\n",
    "## across multiple lines with the paranthesis. Be sure not to add \n",
    "## commas, or it will be interpreted as a tuple!\n",
    "\n",
    "example_string = ('With educated people, I suppose, punctuation is a matter of rule; '\n",
    "                  'with me it is a matter of feeling. But I must say I have a great '\n",
    "                  'respect for the semi-colon; its a useful little chap.')\n",
    "print(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Strings have a number of useful attributes for identifying features of \n",
    "## their content.\n",
    "print('Is \"comma\" in string?: %s' %('orangutan' in example_string) )\n",
    "print('Number of \"a\"s in string?: %s' %example_string.count('a'))\n",
    "print('Index of \"respect\" in string?: %s' %example_string.index('respect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Strings have a number of useful attributes for manipulating their content.\n",
    "print(example_string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Strings have a number of useful attributes for manipulating their content.\n",
    "print(example_string.replace('educated','obnoxious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Strings even have functions to break down and recombine their constituents.\n",
    "split_string = example_string.split(' ')\n",
    "sort_string = ' '.join(sorted(split_string))\n",
    "\n",
    "print(split_string)\n",
    "print(sort_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "Though the basic strings attributes are useful, they can be very clunky for certain tasks. For example, if we wanted to remove every punctuation in the string above, we would have to chain together multiple calls to the replace command. Similarly, if wanted to find every word ending with \"ing\", no command could give us an answer in one step (especially not with punctuation). \n",
    "\n",
    "We briefly introduce regular expressions, a mini-language used to interrogate strings, and highlight some use cases. The [documentation](https://docs.python.org/2/library/re.html) provides many more examples and details for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re    # Import the regular expressions package.\n",
    "\n",
    "## Define pattern: string begins with \"r\". \n",
    "## To break it down, the pattern works in four parts:\n",
    "## 1. \"\\W\" specifies the pattern begins with any non-alphanumeric character.\n",
    "## 2. \"r\" specifies that \"r\" should follow the space.\n",
    "## 3. \"[a-z]\" specifies we're looking for any alphabetical character.\n",
    "## 4. \"*\" specifies we're looking for any number of repeats of the previous.\n",
    "\n",
    "pattern = '\\Wr[a-z]*'\n",
    "print( 'Find strings beginning with \"r\": %s' %re.findall(pattern,example_string) )\n",
    "\n",
    "## Define pattern: string ends with \"s\" or \"t\".\n",
    "## To break it down, the pattern works in five parts.\n",
    "## 1. \"\\W\" specifies the pattern begins with any non-alphanumeric character.\n",
    "## 2. \"[a-z]\" specifies we're looking for any alphabetical character.\n",
    "## 3. \"*\" specifies we're looking for any number of repeats of the previous.\n",
    "## 4. \"[st]\" specifies we're looking for \"s\" or \"t\".\n",
    "## 5. \"\\W\" specifies the pattern ends with any non-alphanumeric character.\n",
    "\n",
    "pattern = '\\W[a-z]*[st]\\W'\n",
    "print( 'Find strings ending with \"s\" or \"t\": %s' %re.findall(pattern,example_string) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "## Find all puncutation.\n",
    "pattern = '[%s]' %string.punctuation\n",
    "\n",
    "print(string.punctuation)\n",
    "print( 'Find all punctuation: %s' %re.findall(pattern,example_string) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find all numeric characters.\n",
    "pattern = '\\d'\n",
    "print('Find all numbers: %s' %re.findall(pattern, 'my favorite numbers are 4, 24, 48'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use regular expressions to find and replace multiple characters as once. This will form the basis of tokenization (discussed in greater detail below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Remove all punctation (i.e. replace with '')\n",
    "pattern = '[%s]' %string.punctuation \n",
    "print(re.sub(pattern, '', example_string) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Text Files\n",
    "Though we have previously shown how to read and save datatables with Pandas, let's briefly discuss how to read and write text files with Python. To do this, we will use the **open** command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open file for writing.\n",
    "write_file = open('example_lincoln.txt', 'w')      # The 'w' stands for write.\n",
    "\n",
    "## Write lines to file.\n",
    "write_file.write( example_string + '\\n' )  # We add a newline character.\n",
    "\n",
    "## Close file for writing.\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also append to a previously written file. Notice the change in syntax below. Using the \"with\" command, we do not need to close the file. All code is executed under the \"with\" block and, once completed, the file is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open file for appending.\n",
    "with open('example_lincoln.txt', 'a') as append_file:\n",
    "    \n",
    "    append_file.write( re.sub(pattern, '', example_string) + '\\n' ) # Append de-punctuated line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now open the file for reading and print the lines in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('example_lincoln.txt', 'r') as read_file:\n",
    "    \n",
    "    for line in read_file.readlines():\n",
    "        \n",
    "        print(line.strip())    # We use the \"strip\" attribute to remove whitespace/line breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: NSF Awards (1970-2016)\n",
    "The National Science Foundation (NSF) has made public the records of every award it has ever granted from 1970 onwards, including the awarding NSF division, the awarded institution, the amount awarded, and, most importantly, the abstract of the awardeed project. In this demonstration we will explore the temporal evolution of scientific topics, as well as the scientific topics that draw the most research funding from the NSF.\n",
    "\n",
    "**NOTE:** The raw data (originally gathered from [here](https://www.nsf.gov/awardsearch/download.jsp)) are stored in XML format. Some code for parsing, reformatting, and saving XML data can be found in the 'nsf' folder in the *nsf_xml_parsing.ipynb* notebook.\n",
    "\n",
    "Before turning to the text data, let's quickly characterize some trends in the funding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import read_csv, concat\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2.5)\n",
    "%matplotlib inline\n",
    "\n",
    "## Define years.\n",
    "years = range(1970,2017)\n",
    "\n",
    "## Iteratively load and concatenate grant dataframes.\n",
    "grants = concat([read_csv(os.path.join('nsf',str(year),'grants.csv')) for year in years])\n",
    "\n",
    "## Fix ID formatting (i.e. prepend 0s). Set as index.\n",
    "grants.ID = grants.ID.apply(lambda i: '%0.7d' %i)\n",
    "grants = grants.set_index('ID')\n",
    "\n",
    "print(grants.shape)\n",
    "grants.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Variables: String Matching\n",
    "Unfortuantely there are inconsistencies in the naming conventions in this dataset. For example, look at the following unique categories in Directorate. It is readily apparent that abbreviations were inconsently used and now certain directorates have multiple names despite the same identity (e.g. 'Direct For Computer & Info Scie & Enginr' and 'Directorate for Computer & Information Science & Engineering'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for title in np.unique( grants.dropna().Directorate ):\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually recode these items with a series of np.where commands, but there may be a more elegant solution with **string matching** techniques. String matching techniques attempt to characterize the similarity of two strings. There are a number of metrics for computing the difference between two strings ([Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance), [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance), [Jaro–Winkler distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance)), but here we will use a pre-installed module built into vanilla play: the **SequenceMatcher** command from difflib. \n",
    "\n",
    "From the SequenceMatcher docstring:\n",
    ">SequenceMatcher is a flexible class for comparing pairs of sequences of any type. The basic algorithm dates back to Ratcliff's and Obershelp's \"gestalt pattern matching\". The basic idea is to find\n",
    "the longest contiguous matching subsequence. The same idea is then applied recursively to the pieces of the sequences to the left and to the right of the matching subsequence.  This does not yield minimal edit sequences, but does tend to yield matches that \"look right\" to people.\n",
    "\n",
    "We can use this algorithm to compute a similarity score between any two strings (e.g. directorate titles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a,b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "print('Silly comparison 1: %0.3f' %similar('the cat in the hat', 'the bat in the vat)'))\n",
    "print('Silly comparison 2: %0.3f' %similar('the cat in the hat', 'the dog on the log)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this algorithm to find the most likely replacements for the abbreviated titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract titles.\n",
    "titles_abbr = [d for d in grants.dropna().Directorate.unique() if d.startswith('Direct ')]\n",
    "titles_full = [d for d in grants.dropna().Directorate.unique() if not d.startswith('Direct ')]\n",
    "\n",
    "## Iteratively identify most similar string.\n",
    "abbr_dict = dict()\n",
    "for abbr in titles_abbr:\n",
    "    abbr_dict[abbr] = titles_full[np.argmax([similar(abbr, full) for full in titles_full])]\n",
    "    \n",
    "## Print.\n",
    "for abbr, full in abbr_dict.items():\n",
    "    print('%s --> %s' %(abbr,full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect matches! We will use this information to update the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Update directorates.\n",
    "for abbr, full in abbr_dict.items():\n",
    "    grants.loc[grants.Directorate==abbr,'Directorate'] = full\n",
    "    \n",
    "## Cheat a little.\n",
    "grants.loc[grants.Directorate=='OFFICE OF THE DIRECTOR','Directorate'] = 'Office Of The Director'\n",
    "grants.loc[grants.Directorate=='Directorate for Geosciences','Directorate'] = 'Directorate For Geosciences'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, we were successfully able to remove all of the duplicates by collapsing across identical directorates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for title in np.unique( grants.dropna().Directorate ):\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends: Total Grants by Year\n",
    "The total number of grants has not changed much since the 70s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "ax = sns.countplot('Year', data=grants, color='#377eb8');\n",
    "ax.set_xticklabels([str(y)[2:] for y in years], fontsize=14);\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('NSF Total Grants by Year')\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends: Total Funding by Year\n",
    "Funding increased steadily until the [Great Recession (2008-2009)](https://www.nsf.gov/news/news_summ.jsp?cntn_id=129264), after which funding has steadily declined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot('Year','Funds',data=grants,estimator=np.sum,ci=None, \n",
    "               kind='bar', color='#377eb8', size=5, aspect=3)\n",
    "g.ax.set_xticklabels([str(y)[2:] for y in years], fontsize=14)\n",
    "g.ax.set_yscale('log')\n",
    "g.ax.set_title('NSF Total Funding by Year');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends: Total Grants by Directorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "order = grants.Directorate.value_counts().index\n",
    "ax = sns.countplot('Directorate', data=grants, color='#377eb8', order=order);\n",
    "ax.set_xticklabels(order,rotation=-60, ha='left', fontsize=14);\n",
    "ax.set_xlabel('')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('NSF Total Grants by Directorate')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends: Average Awarded Amount by Directorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb = grants.groupby('Directorate').Funds.mean()\n",
    "factor_order = gb.index[np.argsort(gb)[::-1]]\n",
    "g = sns.factorplot('Directorate','Funds',data=grants,estimator=np.mean,ci=68, \n",
    "                kind='bar', color='#377eb8', order=factor_order, size=5, aspect=3);\n",
    "g.set_xticklabels(rotation=-60, ha='left', fontsize=14);\n",
    "g.ax.set_xlabel('')\n",
    "g.ax.set_yscale('log')\n",
    "g.ax.set_title('NSF Average Award Amount by Directorate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends: Total Grants Awarded by Institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "order = grants.Institution.value_counts().index\n",
    "ax = sns.countplot('Institution', data=grants, color='#377eb8', order=order[:10]);\n",
    "ax.set_xticklabels(order,rotation=-60, ha='left', fontsize=16);\n",
    "ax.set_xlabel('')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('NSF Total Grants by Institution')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends: Total Funding Awarded by Institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb = grants.groupby('Institution').Funds.sum()\n",
    "order = gb.index[np.argsort(gb)[::-1]]\n",
    "g = sns.factorplot('Institution','Funds',data=grants,estimator=np.sum,ci=None, \n",
    "                kind='bar', color='#377eb8', order=order[:10], size=5, aspect=3);\n",
    "g.set_xticklabels(rotation=-60, ha='left', fontsize=14);\n",
    "g.ax.set_xlabel('')\n",
    "g.ax.set_title('NSF Total Funding Awarded by Institution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "A few general takeaways:\n",
    "1. Though the total number of grants awarded by the NSF has not increased substantially over the past 4 decades, the total amount of funds allocated has steadily increased (up until the 2008-2009 recession at least). \n",
    "2. Not every Granting Directorate is equal: The Directorates of Mathematics, Physics, Engineering, and Biology award the most grants, whereas the Directorates of Scientific Coordination, Geosciences, and  Education award the most on average. The Directorate of Polar Research and Information Management consistently award the least and least often.\n",
    "3. Large public research institutions (e.g. Berkeley, Michigan, Washington) have been awarded the most grants, but are not necessarily the most highly funded institutions. \n",
    "\n",
    "Let us see if the text information encoded in the abstracts can be used to predict research funding, as well as if certain words predict different Directorates/Institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "In the next section we will cover a summary of the steps involved in setting up a preprocessing pipeline for natural language processing tasks. Specifically, we will cover issues concerning text encoding, tokenizing, spell checking, stopwords, and stemming/lemanization. We will a simple pipeline and apply it to the abstracts of NSF awarded grants. \n",
    "\n",
    "**NOTE:** The abstracts for each year are stored in the *abstracts.txt* file in its corresponding  directory organized by year. Within a file, abstracts are separated by line with its corresponding ID at the beginning of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoding\n",
    "The first major hurdle in text processing is identifying the text encoding of the file. The encoding is the machine language detailing how the text is represented under-the-hood. Without specifying the correct encoding, the text cannot be recovered. \n",
    "\n",
    "Fortunately, our abstract data has been encoded in the universal UTF-8. For files not encoded as such, however, the **codecs** module can be used to read and decode files of alternate encodings. To show how this goes wrong, we will try opening a file encoded in UTF-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "## Open file with UTF-8 encoding.\n",
    "with codecs.open('example_utf16.txt', 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    \n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easily fixed when we specify the correct encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Open file with UTF-16 encoding.\n",
    "with codecs.open('example_utf16.txt', 'r', encoding='utf-16') as f:\n",
    "    line = f.readline()\n",
    "    \n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of our demonstration, we will use the first abstract from 2016. Abstracts are stored with their IDs line-by-line. IDs are tab-separated from their corresponding abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Open 2016 abstracts document and read first line (i.e. first abstract).\n",
    "with codecs.open(os.path.join('nsf','2016','abstracts.txt'), 'r', encoding='utf-8') as f:\n",
    "    abstract = f.readline()\n",
    "    \n",
    "## Separate ID and abstract.\n",
    "id, abstract = abstract.split('\\t')\n",
    "\n",
    "## Use regular expressions to remove any remaining XML paragraph break tags.\n",
    "pattern = '<br/>'\n",
    "abstract = re.sub(pattern, '', abstract)\n",
    "\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "The next step is breaking down the abstract into its constituent elements, i.e. words. We do this using the tokenizing function from the Natural Language Toolkit (NLTK). The NLTK library is:\n",
    ">... a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define path for NLTK to search for tokenizer information.\n",
    "nltk.data.path.append(os.path.abspath('nltk_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer accepts regular expression patterns in instructing it what text to preserve and what text to toss. Below we specify to only keep the alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## Tokenize into sentences.\n",
    "sentences = sent_tokenize( abstract )\n",
    "\n",
    "for n, sentence in enumerate(sentences):\n",
    "    print( '%s %s...' %(n,sentence[:70]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "## Define regex inclusion pattern. This pattern accepts\n",
    "## all alphanumeric characters and the hyphen character.\n",
    "regex = '[\\w-]+'\n",
    "\n",
    "## Initialize tokenizer with regular expression.\n",
    "tokenizer = RegexpTokenizer(regex)\n",
    "\n",
    "## Apply to sentences.\n",
    "sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Tagging\n",
    "**Position tagging:** Position tagging is the linguistic process of identifying **parts of speech** (e.g. nouns, verbs, adjectives). NLTK has extensive [documentation](http://www.nltk.org/book/ch05.html) and has many useful tools for decomposing sentences into their respective lexical categories. With this approach, we could identify proper nouns in sentences (NP tag) and drop them. Unfortunately, position tagging is a non-trivial linguistic problem, with many corner cases and errors, and can also be computationally demanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "## Define the positions of speech of the first sentence.\n",
    "pos_tag(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these tags mean? A full list [here](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/).\n",
    "\n",
    "|Class|Type|Tag|Example| \n",
    "|---|---|---|---|\n",
    "|Noun|Singular<br/>Plural<br/>Proper (s)<br/>Proper (p)|NN<br/>NNS<br/>NNP<br/>NNPS|citizen<br/>citizens<br/>American<br/>Americans|\n",
    "|Verb|Base<br/>Past<br/>Gerund<br/>Past participle<br/>Present|VB<br/>VBD<br/>VBG<br/>VBN<br/>VBP|take<br/>took<br/>taking<br/>taken<br/>take|\n",
    "|Adjective|Base<br/>Comparative<br/>Superlative|JJ<br/>JJR<br/>JJS|big<br/>bigger<br/>biggest|\n",
    "|Other|Conjunction<br/>Determiner<br/>Preposition|CC<br/>DD<br/>IN|and<br/>this, that<br/>in, of|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Tokenize all sentences and store with associated POS.\n",
    "tokens_pos = np.vstack([pos_tag(sentence) for sentence in sentences])\n",
    "\n",
    "print(tokens_pos.shape)\n",
    "print(tokens_pos[::15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "One easy way to increase the informational content of your corpus is to remove highly common words (e.g. a, an, and, of, this, that, etc.). These are highly likely to appear in every documents and thus of little explanatory power. Many packages (NLTK, SpaCy, Scikit-Learn) have built-in **stop words**, or a list of the most frequently appearing words for a given language. Below we will use Scikit-Learn's list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "print(stop_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the stoplist to our tokens using the in1d command from NumPy. After applying stopwords, the abstract is reduced by approximately 75 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define function to lowercase all strings.\n",
    "lower = np.vectorize(lambda s: s.lower())\n",
    "\n",
    "## Match all tokens in stoplist.\n",
    "ix = np.in1d(np.apply_along_axis(lower, 0, tokens_pos[:,0]), stop_words)\n",
    "\n",
    "## Remove all stopwords (i.e. those in tokens matching stopwords)\n",
    "tokens_pos = tokens_pos[~ix]\n",
    "\n",
    "print('Total tokens after stop words: %s' %len(tokens_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, stoplists largely remove \"glue\" words (i.e. conjunctives, determiners, prepositions). We can confirm this by looking at the types of tags left over after applying the stop words. As can be seen below, what largely remains are nouns, verbs, adjectives, and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.unique(tokens_pos[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "A second way to increase the informational content of your corpus (though far less simple than utilizing stop words) is through stemming and lemmatizing your tokens. The [Stanford NLP group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) provides the following definitions:\n",
    "* **Stemming:** refers to a process that removes the ends of words, thereby reducing them to their stems. This most frequently includes the removal of derivational affixes (e.g. joyful --> joy).\n",
    "* **Lemmatization:** refers to a process that converts words so as to remove inflectional endings only and to return the base or dictionary form of a word, i.e. its lemma (e.g. walks, walked, walking --> walk). \n",
    "\n",
    "There are several methods of stemming/lemmatizing implemented by different modules and functions. For example, NLTK and SpaCy both include stemming/lemmatizing functions which produce slightly different outputs. We provide examples of some differences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "## Seprate tokens from POS-tags.\n",
    "tokens, tags = tokens_pos.T\n",
    "\n",
    "## Initialize and vectorize functions.\n",
    "ls = np.vectorize(LancasterStemmer().stem)\n",
    "ps = np.vectorize(PorterStemmer().stem)\n",
    "wnl = np.vectorize(WordNetLemmatizer().lemmatize)\n",
    "\n",
    "## Apply and store outputs.\n",
    "outputs = DataFrame(np.vstack([tokens, tags, np.apply_along_axis(ls, 0, tokens), \n",
    "                               np.apply_along_axis(ps, 0, tokens),\n",
    "                               np.apply_along_axis(wnl, 0, tokens)]).T,\n",
    "                     columns=('Token','Tag','Lancaster','Porter','WordNet'))\n",
    "\n",
    "## Compare and print agreement.\n",
    "for col in outputs.columns[2:]:\n",
    "    print('%s agreement: %0.3f' %(col, (outputs['Token']==outputs[col]).mean()))\n",
    "    \n",
    "outputs.iloc[::12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each approach has its own strengths and weaknesses. The WordNetLemmatizer is the most conservative, largely only de-pluralizing certain noun tokens. LancasterStemmer is the most liberal in its pruning, ocassionally removing stems that change the word entirely (e.g. population --> pop, elementary --> el). The PorterStemmer seems to be somewhere in the middle, stemming many words without rendering them unintelligible (e.g. dimension --> dimensional, population --> popul). It is again worth emphasizing that there is no one correct solution and what will work best is very contingent on the corpus and goal of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract all verbs from tokens.\n",
    "verbs = tokens[[True if tag.startswith('V') else False for tag in tags]]\n",
    "\n",
    "## Apply PorterStemmers to verbs.\n",
    "verbs_ps = np.apply_along_axis(ps, 0, verbs)\n",
    "\n",
    "print(verbs[::5])\n",
    "print(verbs_ps[::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract all plural nouns from tokens.\n",
    "nouns = tokens[[True if tag.startswith('N') and tag.endswith('S') else False for tag in tags]]\n",
    "\n",
    "## Apply PorterStemmers to verbs.\n",
    "nouns_wnl = np.apply_along_axis(wnl, 0, nouns)\n",
    "\n",
    "print(nouns)\n",
    "print(nouns_wnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checking\n",
    "Though this step is not strictly necessary, and will not be used below, there are options for spell-checking in python. Peter Norvig has written an incredible [tutorial for spellchecking in python](http://norvig.com/spell-correct.html) to provide an intuition for how it is possible to write by hand with minimal code. We will instead cheat and use the **pyenchant** package, a spellchecking library in Python [(documentation)](https://pythonhosted.org/pyenchant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "## Define the language of use (i.e. American English).\n",
    "## Possible languages include: English, German, French, and more.\n",
    "spellcheck = enchant.Dict('en_US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *check* attribute will lookup a specified string to see if it is correctly spelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( spellcheck.check('Hello') )\n",
    "print( spellcheck.check('Helo') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *suggest* attribute will provide a list of possible correct spellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( spellcheck.suggest('Helo') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, let's check to see which words it does not recognize. As can be seen, the spellchecker breaks down for proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[s for s in tokens if not spellcheck.check(str(s))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to want to replace these with a suggested word, the recommended words for Archimedean are accurate; the first suggested word is its root, Archimedes. This is not terribly surprising given its status as a better-known proper noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spellcheck.suggest('Archimedean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately Calabi (of the less frequently encountered [Calabi–Yau manifold](https://en.wikipedia.org/wiki/Calabi%E2%80%93Yau_manifold)) does not possess the same fame. It's top recommendation is calabash, \"an evergreen tropical American tree that bears fruit in the form of large woody gourds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spellcheck.suggest('Calabi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, codecs, re\n",
    "import nltk, enchant\n",
    "import numpy as np\n",
    "from pandas import read_csv, concat\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define years.\n",
    "years = range(2006,2017)\n",
    "\n",
    "## Define path for NLTK to search for tokenizer information.\n",
    "nltk.data.path.append(os.path.abspath('nltk_data'))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Load grant information.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Iteratively load and concatenate grant dataframes.\n",
    "grants = concat([read_csv(os.path.join('nsf',str(year),'grants.csv')) for year in years])\n",
    "\n",
    "## Fix ID formatting (i.e. prepend 0s). Set as index.\n",
    "grants.ID = grants.ID.apply(lambda i: '%0.7d' %i)\n",
    "grants = grants.set_index('ID')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define functions for stop words.\n",
    "lower = np.vectorize(lambda s: s.lower())\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "## Define functions for pipeline.\n",
    "def parse_abstracts(filepath):\n",
    "    \n",
    "    ## Open file.\n",
    "    with codecs.open(filepath, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        ## Iterate over abstracts.\n",
    "        for line in file.readlines():\n",
    "            \n",
    "            ## Split into ID / abstract.\n",
    "            try:\n",
    "                ID, abstract = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                continue   \n",
    "            \n",
    "            abstract = re.sub('<br/>', '', abstract)           \n",
    "            \n",
    "            yield(ID,abstract)\n",
    "            \n",
    "def tokenize_abstract(abstract, pattern):\n",
    "    \n",
    "    ## Sentence tokenize.\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    \n",
    "    ## Initialize word tokenizer.\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    \n",
    "    ## Word tokenize. \n",
    "    tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    tokens = [sentence for sentence in tokens if sentence]\n",
    "    return(tokens)\n",
    "\n",
    "def pos_tag_abstract(tokens):\n",
    "    tokens, tags = np.vstack([pos_tag(sentence) for sentence in tokens]).T\n",
    "    return(tokens, tags)\n",
    "\n",
    "def stem_verbs(tokens, tags, stemmer=PorterStemmer):\n",
    "    \n",
    "    ## Initialize stemmer.\n",
    "    stemmer = np.vectorize(PorterStemmer().stem)\n",
    "    \n",
    "    ## Find indices of verbs.\n",
    "    ix = [True if tag.startswith('V') else False for tag in tags]\n",
    "    \n",
    "    ## Apply stemmer.\n",
    "    tokens[ix] = np.apply_along_axis(stemmer, 0, tokens[ix])\n",
    "    return(tokens)\n",
    "\n",
    "def lemmatize_nouns(tokens, tags, lemmatizer=WordNetLemmatizer):\n",
    "    \n",
    "    ## Initialize stemmer.\n",
    "    stemmer = np.vectorize(WordNetLemmatizer().lemmatize)\n",
    "    \n",
    "    ## Find indices of verbs.\n",
    "    ix = [True if tag.startswith('N') else False for tag in tags]\n",
    "    \n",
    "    ## Apply stemmer.\n",
    "    tokens[ix] = np.apply_along_axis(stemmer, 0, tokens[ix])\n",
    "    return(tokens)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "IDs = []\n",
    "for year in years:\n",
    "                   \n",
    "    ## Initialize generator.\n",
    "    filepath = os.path.join('nsf', str(year), 'abstracts.txt')\n",
    "    G = parse_abstracts(filepath)\n",
    "    \n",
    "    for ID, abstract in G: \n",
    "        \n",
    "        ## Tokenize abstract.\n",
    "        tokens = tokenize_abstract(abstract, '[\\w-]+')\n",
    "        \n",
    "        ## POS-tag abstract.\n",
    "        tokens, tags = pos_tag_abstract(tokens)\n",
    "\n",
    "        ## Remove all tokens with fewer than 3 characters.\n",
    "        stop_ix = np.array([True if len(token) < 3 else False for token in tokens])\n",
    "        tokens, tags = tokens[~stop_ix], tags[~stop_ix]\n",
    "        if not len(tokens): continue\n",
    "        \n",
    "        ## Apply stopwords.\n",
    "        stop_ix = np.in1d(np.apply_along_axis(lower, 0, tokens), stop_words)\n",
    "        tokens, tags = tokens[~stop_ix], tags[~stop_ix]\n",
    "        if not len(tokens): continue\n",
    "        \n",
    "        ## Remove proper nouns.\n",
    "        stop_ix = np.in1d(tags, ['NNP','NNPS'])\n",
    "        tokens, tags = tokens[~stop_ix], tags[~stop_ix]\n",
    "        if not len(tokens): continue\n",
    "\n",
    "        ## Stem/lemmatize tokens.\n",
    "        ## Try/catch to handle when there are no such tags.\n",
    "        try: tokens = lemmatize_nouns(tokens, tags)\n",
    "        except ValueError: pass\n",
    "        \n",
    "        try: tokens = stem_verbs(tokens, tags)\n",
    "        except ValueError: pass\n",
    "        \n",
    "        ## Write to file.\n",
    "        out_file = os.path.join('nsf','tokenized', '%s.txt' %ID)\n",
    "        with codecs.open(out_file, 'w', encoding='utf-8') as out_file:\n",
    "            line = ' '.join(tokens).lower()\n",
    "            out_file.write(line)\n",
    "            \n",
    "        ## Append ID.\n",
    "        IDs.append(ID)\n",
    "        \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Limit grants to included abstracts.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Restrict.\n",
    "grants = grants.loc[IDs]\n",
    "\n",
    "## Save.\n",
    "grants.to_csv(os.path.join('nsf', 'abstracts_metadata.csv'))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "It is easy to train the same sorts of machine learning models we previously encountered on text data. What is necessary, of course, is converting the text data into some numeric format for modeling purposes. Scikit-Learn has a number of functions for accomplishing this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling Text Data, Part 1\n",
    "The most straightforward way of rescaling text data to numeric format is to simply count the occurrence of words across documents. In doing so, we end up with an NxM matrix, where N is the number of documents and M is the number of unique tokens. The **CountVectorizer** class from Scikit-Learn performs just this function. Before applying CountVectorizer to the NSF data, let's first demonstrate its utility with a toy dataset.\n",
    "\n",
    "Our dataset will consist of 20 features drawn from 2 categories: {A01, A02, A03, ..., A10} and {B01, B02, B03, ..., B10}. To generate this dataset, we will construct a transition matrix which dictates the probability of encountering, in a document, Token2 given Token1. The tokens from category A will have slightly higher probabilities towards other category A members, and likewise for category B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "%matplotlib inline\n",
    "\n",
    "## Define homophily index.\n",
    "hindex = 3.0\n",
    "\n",
    "## Define tokens.\n",
    "tokens = ['A%0.2d' %n for n in np.arange(10)+1] + ['B%0.2d' %n for n in np.arange(10)+1]\n",
    "\n",
    "## Define transition probability matrix.\n",
    "trans_mat = np.hstack( [np.ones((10,10))*hindex, np.ones((10,10))] )\n",
    "trans_mat = np.vstack( [trans_mat, trans_mat[:, ::-1]] )\n",
    "trans_mat[np.diag_indices_from(trans_mat)] = 0\n",
    "trans_mat = np.apply_along_axis(lambda arr: arr / arr.sum(), 1, trans_mat)\n",
    "\n",
    "## Plot.\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = sns.heatmap(trans_mat, square=True, xticklabels=tokens, yticklabels=tokens, \n",
    "                 cbar_kws={'label': 'Transition Probability'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this transition proability matrix generated, we will generate 100 documents by starting a document with each respective token and samping J new tokens based on a random integer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(47404)\n",
    "\n",
    "## Generate 100 random integers.\n",
    "doc_lengths = np.random.randint(2,15,100)\n",
    "\n",
    "## Generate documents.\n",
    "documents = []\n",
    "for token, doc_length in zip(tokens*5, doc_lengths):\n",
    "    \n",
    "    ## Initialize document.\n",
    "    document = [token]\n",
    "    \n",
    "    ## Iteratively add tokens to document.\n",
    "    while len(document) < doc_length:\n",
    "        \n",
    "        ix = tokens.index(document[-1])\n",
    "        document.append( np.random.choice(tokens, 1, p=trans_mat[ix])[0] )\n",
    "        \n",
    "    ## Append document to documents.\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)\n",
    "\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **CountVectorizer** to convert these documents into their numeric representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Initialize CountVectorizer class.\n",
    "cvec = CountVectorizer(lowercase=False)\n",
    "\n",
    "## Fit to documents.\n",
    "cfit = cvec.fit(documents)\n",
    "print(cfit.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use fit_transform to generate counts.\n",
    "cmat = cvec.fit_transform(documents)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot.\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = sns.heatmap(cmat.toarray(), xticklabels=cfit.get_feature_names(), \n",
    "                 yticklabels='', cbar_kws={'label': 'Token Counts'});\n",
    "ax.set_ylabel('Documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Define dependent variable.\n",
    "y = np.array([0 if doc.startswith('A') else 1 for doc in documents])\n",
    "\n",
    "## Initialize logistic regression w/ cross-validation class.\n",
    "lrcv = LogisticRegressionCV(Cs=[1.], cv=5, random_state=0)\n",
    "\n",
    "## Fit.\n",
    "lrf = lrcv.fit(cmat, y)\n",
    "\n",
    "## Print scores.\n",
    "print('Average classification score: %0.2f' %lrf.scores_[1].mean()) \n",
    "\n",
    "## Plot coefficients.\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = sns.barplot(cfit.get_feature_names(), lrf.coef_.flatten(), [token[0] for token in tokens])\n",
    "ax.hlines(0,-0.1,20.1)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "ax.set_title('CV-Logistic Regression Coefficients')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling NSF Data\n",
    "Using all abstracts between 2006 and 2016, let us attempt to predict Directorate category from the tokenized text alone. Just as before, the first step will be to perform frequency counts of the tokens within documents. \n",
    "\n",
    "To produce the frequency counts, we will still use CountVectorizer but we will initialize it to read data separated into files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Initialize CountVectorizer specifying files as inputs.\n",
    "cvec = CountVectorizer(input='filename')\n",
    "\n",
    "## Define all file paths.\n",
    "documents = [os.path.join('nsf/tokenized/%s' %f) for f in sorted(os.listdir('nsf/tokenized/'))]\n",
    "\n",
    "cvec_fit = cvec.fit(documents)\n",
    "cmat = cvec_fit.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because producing the CountVectorizer process can be computationally intensive, and thus slow, we have precumpted the sparse matrix. We will read it in and construct it using NumPy and SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<134244x146663 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16568090 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "## Load compressed numpy file.\n",
    "npz = np.load(os.path.join('nsf','NSF_cvec.npz'))\n",
    "features = npz['features']\n",
    "\n",
    "## Build sparse matrix.\n",
    "cmat = csr_matrix((npz['data'], npz['indices'], npz['indptr']), npz['shape'])\n",
    "cmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2)\n",
    "%matplotlib inline\n",
    "\n",
    "## Compute feature sums (convert into NumPy array).\n",
    "feature_counts = np.array(cmat.sum(axis=0)).squeeze()\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "ax.plot( np.sort(feature_counts)[::-1], linewidth=3 )\n",
    "ax.set_xticks([0,15e4])\n",
    "ax.set_xlabel('Token Rank')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Token Count')\n",
    "ax.set_title('NSF Abstracts (2006-2016)')\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear we are dealing with a power law distribution here. We can visualize the most popular words. We can create a wordcloud visualizing the most common words of in our corpus. (See [here](https://amueller.github.io/word_cloud/index.html) for more examples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "## Construct token-count dictionary.\n",
    "token_counts = dict()\n",
    "for feature, feature_count in zip(features, feature_counts):\n",
    "    token_counts[feature] = feature_count\n",
    "    \n",
    "## Initialize and generate wordcloud from token-counts.\n",
    "wordcloud = WordCloud(background_color='white', \n",
    "                      max_font_size=40).generate_from_frequencies(token_counts)\n",
    "\n",
    "## Print top 10 tokens.\n",
    "for k,v in [(k, token_counts[k]) for k in sorted(token_counts, key=token_counts.get, reverse=True)][:10]:\n",
    "    print('%s: %s' %(k,v))\n",
    "    \n",
    "## Generate \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load in the metadata associated with the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "## Load metadata.\n",
    "metadata = read_csv(os.path.join('nsf','abstracts_metadata.csv'))\n",
    "metadata.Directorate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting the model, we will remove the categories with low counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define major (frequent) directorate categories.\n",
    "directorates = [d for d in metadata.Directorate.astype(str).unique() \n",
    "                if d.startswith('Directorate') or d.endswith('Director')]\n",
    "\n",
    "## Reduce sparse matrix & DataFrame to major categories.\n",
    "ix = np.in1d(metadata.Directorate, directorates)\n",
    "cmat = cmat[ix]\n",
    "metadata = metadata.loc[ix].copy()\n",
    "\n",
    "print(cmat.shape)\n",
    "metadata.Directorate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also remove low frequency tokens. Let's visualize how many singletons and near-singletons there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Digitize frequency counts into bins: 1, 2, 3, ..., 9, >=10\n",
    "bins = np.arange(10)\n",
    "cats = np.digitize(feature_counts, bins, right=True)\n",
    "\n",
    "## Count instances.\n",
    "bins, counts = np.unique(cats, return_counts=True)\n",
    "\n",
    "## Visualize.\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(bins, counts, color='#377eb8')\n",
    "ax.set(xlabel='Token Frequency', ylabel='Count', yticks=[0,2e4,4e4,6e4])\n",
    "ax.set_xticklabels([1,2,3,4,5,6,7,8,9,r'$\\geq$10'])\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove singletons only, i.e. all tokens occurring only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Find all features occurring only once.\n",
    "singleton_ix = np.where(feature_counts==1, True, False) \n",
    "\n",
    "## Remove from feature names and sparse matrix.\n",
    "cmat = cmat[:,~singleton_ix]\n",
    "features = features[~singleton_ix]\n",
    "cmat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting NSF Data: Naive Bayes Classifiers\n",
    "To predict directorate category from our token data, we will rely on Naive Bayes classificiation. Naive Bayes classification is similar to linear model, but are faster in training and especially efficient for sparse data (such as with text data). They achieve this efficiency by representing the relationship of feature to outcome as the average frequency of observation between a feature and outcome. In the case of binary/multinomial classification, this amounts to counting the number of times a feature appears with a given outcome. Given that computing counts (i.e. summing) is incredibly cheap, Naive Bayes classifers can achieve high efficiency even with high dimensional data.\n",
    "\n",
    "Naive Bayes classifiers rely on one parameter: alpha. Alpha, as before, is a sparsity parameter that affects model complexity. Increasing levels of alpha instructs the classification algorithm to add additional observations to all of the features, thereby smoothing counts and reducing model complexity. Performance is relatively insensitive to the setting of alpha; however, tuning it usually improves accuracy somewhat.\n",
    "\n",
    "As before, we will test out several levels of alpha to find an optimal set of model parameters. **NOTE:** This may take several minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "## Define variables.\n",
    "X = cmat\n",
    "y = metadata.Directorate\n",
    "\n",
    "## Define alphas.\n",
    "alphas = np.power(10., np.arange(5))\n",
    "n_alphas = alphas.shape[0]\n",
    "\n",
    "## Initialize cross validation method.\n",
    "n_splits = 10\n",
    "kfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "## Main loop.\n",
    "scores = np.empty((n_alphas, n_splits))\n",
    "for n in range(n_alphas):\n",
    "    \n",
    "    ## Initialize MultinomialNB\n",
    "    mnb = MultinomialNB(alpha=alphas[n])\n",
    "    \n",
    "    ## Compute scores.\n",
    "    scores[n] = cross_val_score(mnb, X, y, cv=kfold)\n",
    "\n",
    "## Assemble into DataFrame. Melt.\n",
    "df = DataFrame(scores.T, columns=alphas)\n",
    "df = df.melt(var_name='Alpha', value_name='Score')\n",
    "\n",
    "## Plot.\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.swarmplot('Alpha','Score',data=df)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the out-of-the-box Naive Bayes classifier (i.e. alpha = 1) performs close to the best, if not the best. In any case, higher sparisity values dramatically worsen model performance. Using an 80/20 split, let's fit a classifier to the data and see which tokens best predict each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Split data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "## Initialize MultinomialNB\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "\n",
    "## Fit model.\n",
    "mnb_fit = mnb.fit(X_train, y_train)\n",
    "\n",
    "## Print scores.\n",
    "print('MultinomialNB train: score = %0.3f' %mnb_fit.score(X_train,y_train))\n",
    "print('MultinomialNB test: score = %0.3f' %mnb_fit.score(X_test,y_test))\n",
    "\n",
    "print('Feature count matrix: (%s, %s)' %mnb_fit.feature_count_.shape)\n",
    "mnb_fit.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract most common features per category.\n",
    "for n, feature_class in enumerate(mnb_fit.classes_):\n",
    "    \n",
    "    ## Get sorted indices of counts.\n",
    "    ix = np.argsort(mnb_fit.feature_count_[n])[::-1]\n",
    "    \n",
    "    ## Print feature class + top 10 features.\n",
    "    print(feature_class)\n",
    "    print(' '.join(features[ix][:10]) + '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the top words per feature class are mixed in terms of their representativeness. Certain tokens seem fairly representative of the top-10 lists to which they belong (e.g. [\"plant\", \"specie\", \"gene\"] for Biological Sciences; [\"Education\", \"Teacher'] for Education and Human Resources). Other tokens appear across the top-10 list for every outcome class and are hardly uniquely representative (e.g. research, project, student). Below, we address some methods of improving this.\n",
    "\n",
    "### Rescaling Text Data, Part 2\n",
    "Beyond simple token-frequency scaling, CountVectorizer also allows for **n-gram tokenization.** In n-gram tokenzation, N-pairs of tokens are counted. This can be useful for capturing recurring phrases (e.g. \"chaos theory\", \"dynamical systems\", \"decision making\". This can be set with the ngram_range flag.\n",
    "\n",
    "A second way of scaling the text employs the **term frequency–inverse document frequency (tf-idf)** method. From Introduction to Machine Learning with Python:\n",
    ">The intuition of this method is to give high weight to a term that appears\n",
    "often in a particular document, but not in many documents in the corpus. If a word\n",
    "appears often in a particular document, but not in very many documents, it is likely\n",
    "to be very descriptive of the content of that document.\n",
    "\n",
    "Scikit-learn implements the tf-idf method in two classes, the **TfidfTransformer**,\n",
    "which takes in the sparse matrix output produced by CountVectorizer and transforms it, or **TfidfVectorizer**, which takes in the text data and does both the bag-of-words\n",
    "feature extraction and the tf-idf transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 70441)\t0.113637674138\n",
      "  (0, 120754)\t0.202854530521\n",
      "  (0, 125075)\t0.0838692649594\n",
      "  (0, 89630)\t0.291782184402\n",
      "  (0, 55271)\t0.0455057467047\n",
      "  (0, 111429)\t0.135244837884\n",
      "  (0, 117220)\t0.0759391518063\n",
      "  (0, 78014)\t0.105373551151\n",
      "  (0, 124512)\t0.140751945834\n",
      "  (0, 39420)\t0.0890407222174\n",
      "  (0, 86048)\t0.0841323981938\n",
      "  (0, 107687)\t0.212514989274\n",
      "  (0, 44772)\t0.346016980525\n",
      "  (0, 60256)\t0.215655927431\n",
      "  (0, 120530)\t0.0778200152808\n",
      "  (0, 121130)\t0.0466794657949\n",
      "  (0, 22340)\t0.164603068423\n",
      "  (0, 144249)\t0.0446698649464\n",
      "  (0, 117225)\t0.0824308011862\n",
      "  (0, 52494)\t0.0522343506536\n",
      "  (0, 37803)\t0.0514224680389\n",
      "  (0, 111620)\t0.0487696173513\n",
      "  (0, 133972)\t0.0669435303252\n",
      "  (0, 124880)\t0.0558466794757\n",
      "  (0, 43763)\t0.0296783622264\n",
      "  :\t:\n",
      "  (134243, 78906)\t0.0909763412385\n",
      "  (134243, 145156)\t0.0837730056086\n",
      "  (134243, 61019)\t0.131948252761\n",
      "  (134243, 76582)\t0.102985775905\n",
      "  (134243, 31828)\t0.0904026823137\n",
      "  (134243, 136175)\t0.109341027497\n",
      "  (134243, 29831)\t0.149211122458\n",
      "  (134243, 55627)\t0.133868263661\n",
      "  (134243, 5843)\t0.11399988676\n",
      "  (134243, 104200)\t0.1677538095\n",
      "  (134243, 79307)\t0.0916864366948\n",
      "  (134243, 115758)\t0.118833380287\n",
      "  (134243, 117689)\t0.10909816149\n",
      "  (134243, 141770)\t0.116147394906\n",
      "  (134243, 113449)\t0.106285868846\n",
      "  (134243, 83947)\t0.120274652421\n",
      "  (134243, 62055)\t0.110623691451\n",
      "  (134243, 97551)\t0.128773623516\n",
      "  (134243, 9380)\t0.143347151063\n",
      "  (134243, 16571)\t0.173016861623\n",
      "  (134243, 119841)\t0.183467454655\n",
      "  (134243, 53853)\t0.182995697075\n",
      "  (134243, 102355)\t0.228868429011\n",
      "  (134243, 102418)\t0.198131829553\n",
      "  (134243, 12367)\t0.444359022177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "## Initialize TfidfTransformer.\n",
    "transformer = TfidfTransformer( )\n",
    "\n",
    "## Apply to sparse CountVec matrix.\n",
    "tfidf_mat = transformer.fit_transform(cmat)\n",
    "print(tfidf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's directly compare the performance of the two means of normalizing the data.\n",
    "\n",
    "**NOTE:** This may take a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize cross validation method.\n",
    "n_splits = 20\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=0)\n",
    "    \n",
    "## Initialize MultinomialNB\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "    \n",
    "## Compute scores.\n",
    "scores = np.empty((n_splits,2))\n",
    "scores[:,0]= cross_val_score(mnb, cmat, y, cv=kfold)         # CountVec\n",
    "scores[:,1]= cross_val_score(mnb, tfidf_mat, y, cv=kfold)    # TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Assemble into DataFrame. Melt.\n",
    "df = DataFrame(scores, columns=['CountVec','TF-IDF'])\n",
    "df = df.melt(var_name='Method', value_name='Score')\n",
    "\n",
    "## Plot.\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.boxplot('Method','Score',data=df)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the most representative tokens now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define variables.\n",
    "X = tfidf_mat\n",
    "y = metadata.Directorate\n",
    "\n",
    "## Split data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "## Initialize MultinomialNB\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "\n",
    "## Fit model.\n",
    "mnb_fit = mnb.fit(X_train, y_train)\n",
    "\n",
    "## Print scores.\n",
    "print('MultinomialNB train: score = %0.3f' %mnb_fit.score(X_train,y_train))\n",
    "print('MultinomialNB test: score = %0.3f' %mnb_fit.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Extract most common features per category.\n",
    "for n, feature_class in enumerate(mnb_fit.classes_):\n",
    "    \n",
    "    ## Get sorted indices of counts.\n",
    "    ix = np.argsort(mnb_fit.feature_count_[n])[::-1]\n",
    "    \n",
    "    ## Print feature class + top 10 features.\n",
    "    print(feature_class)\n",
    "    print(' '.join(features[ix][:10]) + '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, many of the tokens of each directorates' top-10 list are more representative of its respective discipline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=100, learning_method=\"online\", random_state=0)\n",
    "document_topics = lda.fit_transform(tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=100, learning_method=\"online\", random_state=0)\n",
    "lda_fit = lda.fit(tfidf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "814px",
    "left": "0px",
    "right": "1009px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
